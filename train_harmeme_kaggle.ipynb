{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle notebook\n",
    "This is the original link of our kaggle notebook that you can find every training that we are running following with the logs and output: https://www.kaggle.com/code/thanhduycao/ediss-ds-lab3.\n",
    "\n",
    "To view the results, you click the \"Show versions\" right next to \"Save Version\" section and you can found it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Import\n",
    "First, you need to import a dataset and a info file from Kaggle Dataset.\n",
    "The dataset and info file from Kaggle Dataset are as follow:\n",
    "- https://www.kaggle.com/datasets/duycaothanh/harmeme/data\n",
    "- https://www.kaggle.com/datasets/duycaothanh/harmeme-info-revised/data\n",
    "\n",
    "Upload this notebook to Kaggle and search for the data above within the Input section and Kaggle will automatically download it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T04:08:04.568190Z",
     "iopub.status.busy": "2025-11-05T04:08:04.567769Z",
     "iopub.status.idle": "2025-11-05T04:08:05.196625Z",
     "shell.execute_reply": "2025-11-05T04:08:05.195926Z",
     "shell.execute_reply.started": "2025-11-05T04:08:04.568170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/SiddhantBikram/MemeCLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T04:08:06.766116Z",
     "iopub.status.busy": "2025-11-05T04:08:06.765572Z",
     "iopub.status.idle": "2025-11-05T04:09:20.137791Z",
     "shell.execute_reply": "2025-11-05T04:09:20.137029Z",
     "shell.execute_reply.started": "2025-11-05T04:08:06.766088Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning openai-clip yacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T04:09:23.251948Z",
     "iopub.status.busy": "2025-11-05T04:09:23.251362Z",
     "iopub.status.idle": "2025-11-05T04:09:23.259128Z",
     "shell.execute_reply": "2025-11-05T04:09:23.258340Z",
     "shell.execute_reply.started": "2025-11-05T04:09:23.251925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/MemeCLIP.py\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from clip import clip\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "torch.set_default_dtype(torch.float32)\n",
    "from models import LinearClassifier, CosineClassifier, LinearProjection, CLIP_Text, Adapter\n",
    "\n",
    "class MemeCLIP(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.acc = torchmetrics.Accuracy(task='multiclass', num_classes = cfg.num_classes)\n",
    "        self.auroc = torchmetrics.AUROC(task='multiclass', num_classes = cfg.num_classes)\n",
    "        self.f1 = torchmetrics.F1Score(task='multiclass', num_classes = cfg.num_classes, average='macro')\n",
    "\n",
    "        self.clip_model, _ = clip.load(self.cfg.clip_variant, device=\"cuda\", jit=False)\n",
    "        self.clip_model.float()\n",
    "\n",
    "        pre_output_input_dim = self.cfg.map_dim\n",
    "        pre_output_layers = [nn.Dropout(p=cfg.drop_probs[1])]\n",
    "        output_input_dim = pre_output_input_dim\n",
    "\n",
    "        self.classifier = CosineClassifier(feat_dim = output_input_dim, num_classes=cfg.num_classes, dtype=self.clip_model.dtype)\n",
    "        self.init_head_text_feat()\n",
    "        self.text_encoder =  CLIP_Text(self.clip_model)\n",
    "        self.img_adapter = Adapter(self.cfg.map_dim, 4).to(self.clip_model.dtype)\n",
    "        self.text_adapter = Adapter(self.cfg.map_dim, 4).to(self.clip_model.dtype)\n",
    "        self.clip_model.visual.proj = None\n",
    "\n",
    "        for _, p in self.clip_model.named_parameters():\n",
    "            p.requires_grad_(False)\n",
    "        \n",
    "        for name, param in self.classifier.named_parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        self.image_map = LinearProjection(self.cfg.unmapped_dim, self.cfg.map_dim,\n",
    "                                          self.cfg.num_mapping_layers, self.cfg.drop_probs)\n",
    "        self.text_map = LinearProjection(self.cfg.unmapped_dim, self.cfg.map_dim,\n",
    "                                         self.cfg.num_mapping_layers, self.cfg.drop_probs)\n",
    "        \n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "            \n",
    "        if self.cfg.num_pre_output_layers >= 1:\n",
    "            pre_output_layers.extend(\n",
    "                [nn.Linear(pre_output_input_dim, self.cfg.map_dim), nn.ReLU(), nn.Dropout(p=cfg.drop_probs[2])])\n",
    "            output_input_dim = self.cfg.map_dim\n",
    "\n",
    "        for _ in range(1, self.cfg.num_pre_output_layers):\n",
    "            pre_output_layers.extend(\n",
    "                [nn.Linear(self.cfg.map_dim, self.cfg.map_dim), nn.ReLU(), nn.Dropout(p=cfg.drop_probs[2])])\n",
    "\n",
    "        self.pre_output = nn.Sequential(*pre_output_layers)\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    def forward(self, batch):\n",
    "        pass\n",
    "    \n",
    "    def init_head_text_feat(self):\n",
    "\n",
    "        print(\"Initialize head with text features\")\n",
    "        template = \"a photo of a {}.\"\n",
    "        prompts = [template.format(c.replace(\"_\", \" \")) for c in self.cfg.class_names]\n",
    "        prompts = clip.tokenize([p for p in prompts], context_length=77, truncate=True).to(self.cfg.device)\n",
    "        text_features = self.clip_model.encode_text(prompts)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "        text_features = text_features @ self.clip_model.visual.proj.t()\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "        self.classifier.apply_weight(text_features)\n",
    "\n",
    "    def common_step(self, batch):\n",
    "\n",
    "        image_embeds = batch['image_features']\n",
    "        text_embeds = batch['text_features']\n",
    "\n",
    "        image_projection = self.image_map(image_embeds)\n",
    "        txt_projection = self.text_map(text_embeds)\n",
    "\n",
    "        image_features = self.img_adapter(image_projection)\n",
    "        text_features = self.text_adapter(txt_projection)\n",
    "\n",
    "        text_features = self.cfg.ratio  * text_features + (1 - self.cfg.ratio ) * txt_projection\n",
    "        image_features = self.cfg.ratio  * image_features + (1 - self.cfg.ratio ) * image_projection\n",
    "\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        features = torch.mul(image_features, text_features)\n",
    "\n",
    "        features_pre_output = self.pre_output(features)\n",
    "        logits = self.classifier(features_pre_output).squeeze(dim=1) \n",
    "        preds_proxy = torch.sigmoid(logits)\n",
    "        _ , preds = logits.data.max(1)\n",
    "\n",
    "        output = {}\n",
    "        output['loss'] = self.cross_entropy_loss(logits, batch['labels'])\n",
    "        output['accuracy'] = self.acc(preds, batch['labels'])\n",
    "        output['auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
    "        output['f1'] = self.f1(preds, batch['labels'])\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.common_step(batch)\n",
    "\n",
    "        total_loss = output['loss']\n",
    "\n",
    "        self.log('train/total_loss', total_loss)\n",
    "        self.log('train/loss', output['loss'])\n",
    "        self.log('train/accuracy', output['accuracy'])\n",
    "        self.log(f'train/auroc', output['auroc'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.common_step(batch)\n",
    "\n",
    "        total_loss = output['loss']\n",
    "\n",
    "        self.log(f'val/total_loss', total_loss)\n",
    "        self.log(f'val/loss', output['loss'])\n",
    "        self.log(f'val/accuracy', output['accuracy'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(f'val/auroc', output['auroc'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(f'val/f1', output['f1'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        output = self.common_step(batch)\n",
    "        self.log(f'test/accuracy', output['accuracy'])\n",
    "        self.log(f'test/auroc', output['auroc'])\n",
    "        self.log(f'test/f1', output['f1'])\n",
    "\n",
    "        return output\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.acc.reset()\n",
    "        self.auroc.reset()\n",
    "        self.f1.reset()\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        self.acc.reset()\n",
    "        self.auroc.reset()\n",
    "        self.f1.reset()\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.acc.reset()\n",
    "        self.auroc.reset()\n",
    "        self.f1.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "            {\"params\": [p for n, p in self.named_parameters() if p.requires_grad]}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.cfg.lr, weight_decay=self.cfg.weight_decay)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "def create_model(cfg):\n",
    "    model = MemeCLIP(cfg)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T04:32:25.428959Z",
     "iopub.status.busy": "2025-11-05T04:32:25.428697Z",
     "iopub.status.idle": "2025-11-05T04:32:25.435083Z",
     "shell.execute_reply": "2025-11-05T04:32:25.434259Z",
     "shell.execute_reply.started": "2025-11-05T04:32:25.428941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/datasets.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import clip\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from configs import cfg\n",
    "from transformers import AutoProcessor, CLIPVisionModel\n",
    "\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "from configs import cfg\n",
    "\n",
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, cfg, root_folder, dataset, label, split='train', image_size=224):\n",
    "        super(Custom_Dataset, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.root_folder = root_folder\n",
    "        self.dataset = dataset\n",
    "        self.split = split\n",
    "        self.label = label\n",
    "\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.info_file = cfg.info_file\n",
    "        self.df = pd.read_csv(self.info_file)\n",
    "        self.df = self.df[self.df['split'] == self.split].reset_index(drop=True)\n",
    "\n",
    "        if self.label == 'target':\n",
    "            self.df = self.df[self.df['harm'] >= 1].reset_index(drop=True)\n",
    "\n",
    "        float_cols = self.df.select_dtypes(float).columns\n",
    "        self.df[float_cols] = self.df[float_cols].fillna(-1).astype('Int64')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        if row['text'] == 'None':\n",
    "            txt = 'null'\n",
    "        else:\n",
    "            txt = row['text']\n",
    "\n",
    "        image_fn = row['name']\n",
    "        image = Image.open(f\"{self.cfg.img_folder}/{image_fn}\").convert('RGB')\\\n",
    "            .resize((self.image_size, self.image_size))\n",
    "        text = txt\n",
    "\n",
    "        item = {\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'label': row[self.label],\n",
    "            'idx_meme': row['name'],\n",
    "            'origin_text': txt\n",
    "        }\n",
    "\n",
    "        return item\n",
    "\n",
    "class Custom_Collator(object):\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.clip_model, _ = clip.load(self.cfg.clip_variant, device=\"cuda\", jit=False)\n",
    "        _, self.clip_preprocess = clip.load(self.cfg.clip_variant, device=\"cuda\", jit=False)\n",
    "        self.clip_model.float().eval()\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        labels = torch.LongTensor([item['label'] for item in batch])\n",
    "        idx_memes = [item['idx_meme'] for item in batch]\n",
    "\n",
    "        batch_new = {'labels': labels,\n",
    "                     'idx_memes': idx_memes,\n",
    "                     }\n",
    "        \n",
    "        image_embed_list = []\n",
    "        text_embed_list = []\n",
    "\n",
    "        for item in batch:\n",
    "\n",
    "            pixel_values = self.clip_preprocess(item['image']).unsqueeze(0)\n",
    "            text = clip.tokenize(item['text'], context_length=77, truncate=True)\n",
    "\n",
    "            image_features, text_features = self.compute_CLIP_features_without_proj(self.clip_model,\n",
    "                                                                    pixel_values.to(self.cfg.device),\n",
    "                                                                    text.to(self.cfg.device))\n",
    "            text_embed_list.append(text_features.cpu().detach())\n",
    "            image_embed_list.append(image_features.cpu().detach())\n",
    "\n",
    "        image_features = torch.cat([item for item in image_embed_list], dim=0)\n",
    "        text_features = torch.cat([item for item in text_embed_list], dim=0)\n",
    "\n",
    "        batch_new['image_features'] = image_features\n",
    "        batch_new['text_features'] = text_features\n",
    "\n",
    "        return batch_new\n",
    "    \n",
    "    def compute_CLIP_features_without_proj(self, clip_model, img_input, text_input):\n",
    "        image_features = clip_model.visual(img_input.type(clip_model.dtype))\n",
    "\n",
    "        x = clip_model.token_embedding(text_input).type(clip_model.dtype)\n",
    "        x = x + clip_model.positional_embedding.type(clip_model.dtype)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = clip_model.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = clip_model.ln_final(x).type(clip_model.dtype)\n",
    "        text_features = x[torch.arange(x.shape[0]), text_input.argmax(dim=-1)]\n",
    "\n",
    "        return image_features, text_features\n",
    "\n",
    "\n",
    "def load_dataset(cfg, split):\n",
    "    dataset = Custom_Dataset(cfg = cfg, root_folder=cfg.root_dir, dataset=cfg.dataset_name, split=split,\n",
    "                           image_size=cfg.image_size, label = cfg.label)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harm Classification Seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/harmeme/harmeme/images'\n",
    "cfg.info_file = '/kaggle/input/harmeme-info-revised/merged_split_labels_numeric.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_harm_42')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Harm'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'harm'\n",
    "cfg.seed = 42\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['Society', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "elif cfg.label == 'harm':\n",
    "    cfg.class_names = ['Not Harm', 'Somewhat Harmful', 'Very Harmful']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 1e-4 # 1e-5, 3e-4, 1e-3\n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T04:41:56.140489Z",
     "iopub.status.busy": "2025-11-05T04:41:56.140237Z",
     "iopub.status.idle": "2025-11-05T04:45:15.692881Z",
     "shell.execute_reply": "2025-11-05T04:45:15.692174Z",
     "shell.execute_reply.started": "2025-11-05T04:41:56.140471Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harm Classification Seed 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/harmeme/harmeme/images'\n",
    "cfg.info_file = '/kaggle/input/harmeme-info-revised/merged_split_labels_numeric.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_harm_100')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Harm'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'harm'\n",
    "cfg.seed = 100\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['Society', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "elif cfg.label == 'harm':\n",
    "    cfg.class_names = ['Not Harm', 'Somewhat Harmful', 'Very Harmful']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 1e-4 # 1e-5, 3e-4, 1e-3\n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harm Classification Seed 510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/harmeme/harmeme/images'\n",
    "cfg.info_file = '/kaggle/input/harmeme-info-revised/merged_split_labels_numeric.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_harm_510')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Harm'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'harm'\n",
    "cfg.seed = 510\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['Society', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "elif cfg.label == 'harm':\n",
    "    cfg.class_names = ['Not Harm', 'Somewhat Harmful', 'Very Harmful']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 1e-4 # 1e-5, 3e-4, 1e-3\n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Classification Seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/harmeme/harmeme/images'\n",
    "cfg.info_file = '/kaggle/input/harmeme-info-revised/merged_split_labels_numeric.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_target_42')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Harm'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'target'\n",
    "cfg.seed = 42\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['Society', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "elif cfg.label == 'harm':\n",
    "    cfg.class_names = ['Not Harm', 'Somewhat Harmful', 'Very Harmful']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 1e-4 # 1e-5, 3e-4, 1e-3\n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Classification Seed 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/harmeme/harmeme/images'\n",
    "cfg.info_file = '/kaggle/input/harmeme-info-revised/merged_split_labels_numeric.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_target_100')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Harm'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'target'\n",
    "cfg.seed = 100\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['Society', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "elif cfg.label == 'harm':\n",
    "    cfg.class_names = ['Not Harm', 'Somewhat Harmful', 'Very Harmful']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 1e-4 # 1e-5, 3e-4, 1e-3\n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Classification Seed 510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/harmeme/harmeme/images'\n",
    "cfg.info_file = '/kaggle/input/harmeme-info-revised/merged_split_labels_numeric.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_target_510')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Harm'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'target'\n",
    "cfg.seed = 510\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['Society', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "elif cfg.label == 'harm':\n",
    "    cfg.class_names = ['Not Harm', 'Somewhat Harmful', 'Very Harmful']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 1e-4 # 1e-5, 3e-4, 1e-3\n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7527542,
     "sourceId": 11970634,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8628266,
     "sourceId": 13580981,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8654370,
     "sourceId": 13618008,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8654462,
     "sourceId": 13618158,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8654545,
     "sourceId": 13618269,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
