{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original link of our kaggle notebook that you can find every training that we are running following with the logs and output: https://www.kaggle.com/code/thanhduycao/ediss-ds-lab3.\n",
    "\n",
    "To view the results, you click the \"Show versions\" right next to \"Save Version\" section and you can found it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Import\n",
    "First, you need to import a dataset and a info file from Kaggle Dataset.\n",
    "The dataset and info file from Kaggle Dataset are as follow:\n",
    "- https://www.kaggle.com/datasets/sundess/pridemm\n",
    "- https://www.kaggle.com/datasets/thanhduycao/pridemm-info\n",
    "\n",
    "Upload this notebook to Kaggle and search for the data above within the Input section and Kaggle will automatically download it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T15:17:49.004464Z",
     "iopub.status.busy": "2025-11-03T15:17:49.004176Z",
     "iopub.status.idle": "2025-11-03T15:17:49.823642Z",
     "shell.execute_reply": "2025-11-03T15:17:49.822983Z",
     "shell.execute_reply.started": "2025-11-03T15:17:49.004441Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MemeCLIP'...\n",
      "remote: Enumerating objects: 94, done.\u001b[K\n",
      "remote: Counting objects: 100% (94/94), done.\u001b[K\n",
      "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
      "remote: Total 94 (delta 30), reused 14 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (94/94), 228.31 KiB | 3.08 MiB/s, done.\n",
      "Resolving deltas: 100% (30/30), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/SiddhantBikram/MemeCLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T20:24:38.650001Z",
     "iopub.status.busy": "2025-11-01T20:24:38.649658Z",
     "iopub.status.idle": "2025-11-01T20:25:58.823495Z",
     "shell.execute_reply": "2025-11-01T20:25:58.822527Z",
     "shell.execute_reply.started": "2025-11-01T20:24:38.649971Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning openai-clip yacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T20:41:08.664298Z",
     "iopub.status.busy": "2025-11-01T20:41:08.663989Z",
     "iopub.status.idle": "2025-11-01T20:41:08.673071Z",
     "shell.execute_reply": "2025-11-01T20:41:08.672241Z",
     "shell.execute_reply.started": "2025-11-01T20:41:08.664274Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/MemeCLIP.py\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from clip import clip\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "torch.set_default_dtype(torch.float32)\n",
    "from models import LinearClassifier, CosineClassifier, LinearProjection, CLIP_Text, Adapter\n",
    "\n",
    "class MemeCLIP(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.acc = torchmetrics.Accuracy(task='multiclass', num_classes = cfg.num_classes)\n",
    "        self.auroc = torchmetrics.AUROC(task='multiclass', num_classes = cfg.num_classes)\n",
    "        self.f1 = torchmetrics.F1Score(task='multiclass', num_classes = cfg.num_classes, average='macro')\n",
    "\n",
    "        self.clip_model, _ = clip.load(self.cfg.clip_variant, device=\"cuda\", jit=False)\n",
    "        self.clip_model.float()\n",
    "\n",
    "        pre_output_input_dim = self.cfg.map_dim\n",
    "        pre_output_layers = [nn.Dropout(p=cfg.drop_probs[1])]\n",
    "        output_input_dim = pre_output_input_dim\n",
    "\n",
    "        self.classifier = CosineClassifier(feat_dim = output_input_dim, num_classes=cfg.num_classes, dtype=self.clip_model.dtype)\n",
    "        self.init_head_text_feat()\n",
    "        self.text_encoder =  CLIP_Text(self.clip_model)\n",
    "        self.img_adapter = Adapter(self.cfg.map_dim, 4).to(self.clip_model.dtype)\n",
    "        self.text_adapter = Adapter(self.cfg.map_dim, 4).to(self.clip_model.dtype)\n",
    "        self.clip_model.visual.proj = None\n",
    "\n",
    "        for _, p in self.clip_model.named_parameters():\n",
    "            p.requires_grad_(False)\n",
    "        \n",
    "        for name, param in self.classifier.named_parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        self.image_map = LinearProjection(self.cfg.unmapped_dim, self.cfg.map_dim,\n",
    "                                          self.cfg.num_mapping_layers, self.cfg.drop_probs)\n",
    "        self.text_map = LinearProjection(self.cfg.unmapped_dim, self.cfg.map_dim,\n",
    "                                         self.cfg.num_mapping_layers, self.cfg.drop_probs)\n",
    "        \n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "            \n",
    "        if self.cfg.num_pre_output_layers >= 1:\n",
    "            pre_output_layers.extend(\n",
    "                [nn.Linear(pre_output_input_dim, self.cfg.map_dim), nn.ReLU(), nn.Dropout(p=cfg.drop_probs[2])])\n",
    "            output_input_dim = self.cfg.map_dim\n",
    "\n",
    "        for _ in range(1, self.cfg.num_pre_output_layers):\n",
    "            pre_output_layers.extend(\n",
    "                [nn.Linear(self.cfg.map_dim, self.cfg.map_dim), nn.ReLU(), nn.Dropout(p=cfg.drop_probs[2])])\n",
    "\n",
    "        self.pre_output = nn.Sequential(*pre_output_layers)\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    def forward(self, batch):\n",
    "        pass\n",
    "    \n",
    "    def init_head_text_feat(self):\n",
    "\n",
    "        print(\"Initialize head with text features\")\n",
    "        template = \"a photo of a {}.\"\n",
    "        prompts = [template.format(c.replace(\"_\", \" \")) for c in self.cfg.class_names]\n",
    "        prompts = clip.tokenize([p for p in prompts], context_length=77, truncate=True).to(self.cfg.device)\n",
    "        text_features = self.clip_model.encode_text(prompts)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "        text_features = text_features @ self.clip_model.visual.proj.t()\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "        self.classifier.apply_weight(text_features)\n",
    "\n",
    "    def common_step(self, batch):\n",
    "\n",
    "        image_embeds = batch['image_features']\n",
    "        text_embeds = batch['text_features']\n",
    "\n",
    "        image_projection = self.image_map(image_embeds)\n",
    "        txt_projection = self.text_map(text_embeds)\n",
    "\n",
    "        image_features = self.img_adapter(image_projection)\n",
    "        text_features = self.text_adapter(txt_projection)\n",
    "\n",
    "        text_features = self.cfg.ratio  * text_features + (1 - self.cfg.ratio ) * txt_projection\n",
    "        image_features = self.cfg.ratio  * image_features + (1 - self.cfg.ratio ) * image_projection\n",
    "\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        features = torch.mul(image_features, text_features)\n",
    "\n",
    "        features_pre_output = self.pre_output(features)\n",
    "        logits = self.classifier(features_pre_output).squeeze(dim=1) \n",
    "        preds_proxy = torch.sigmoid(logits)\n",
    "        _ , preds = logits.data.max(1)\n",
    "\n",
    "        output = {}\n",
    "        output['loss'] = self.cross_entropy_loss(logits, batch['labels'])\n",
    "        output['accuracy'] = self.acc(preds, batch['labels'])\n",
    "        output['auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
    "        output['f1'] = self.f1(preds, batch['labels'])\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.common_step(batch)\n",
    "\n",
    "        total_loss = output['loss']\n",
    "\n",
    "        self.log('train/total_loss', total_loss)\n",
    "        self.log('train/loss', output['loss'])\n",
    "        self.log('train/accuracy', output['accuracy'])\n",
    "        self.log(f'train/auroc', output['auroc'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.common_step(batch)\n",
    "\n",
    "        total_loss = output['loss']\n",
    "\n",
    "        self.log(f'val/total_loss', total_loss)\n",
    "        self.log(f'val/loss', output['loss'])\n",
    "        self.log(f'val/accuracy', output['accuracy'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(f'val/auroc', output['auroc'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(f'val/f1', output['f1'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        output = self.common_step(batch)\n",
    "        self.log(f'test/accuracy', output['accuracy'])\n",
    "        self.log(f'test/auroc', output['auroc'])\n",
    "        self.log(f'test/f1', output['f1'])\n",
    "\n",
    "        return output\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.acc.reset()\n",
    "        self.auroc.reset()\n",
    "        self.f1.reset()\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        self.acc.reset()\n",
    "        self.auroc.reset()\n",
    "        self.f1.reset()\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.acc.reset()\n",
    "        self.auroc.reset()\n",
    "        self.f1.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "            {\"params\": [p for n, p in self.named_parameters() if p.requires_grad]}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.cfg.lr, weight_decay=self.cfg.weight_decay)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "def create_model(cfg):\n",
    "    model = MemeCLIP(cfg)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hate Classification Seed 42 Lr 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/pridemm/PrideMM/Images'\n",
    "cfg.info_file = '/kaggle/input/pridemm-info/PrideMM.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_hate_lr3e4')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Pride'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'hate'\n",
    "cfg.seed = 42\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['No particular target', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 3e-4\n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-02T03:14:12.565Z",
     "iopub.execute_input": "2025-11-01T20:43:07.125020Z",
     "iopub.status.busy": "2025-11-01T20:43:07.124448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hate Classification Seed 42 Lr 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/pridemm/PrideMM/Images'\n",
    "cfg.info_file = '/kaggle/input/pridemm-info/PrideMM.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_hate_lr1e5')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Pride'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'hate'\n",
    "cfg.seed = 42\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['No particular target', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 1e-5 \n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hate Classification Seed 42 Lr 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/pridemm/PrideMM/Images'\n",
    "cfg.info_file = '/kaggle/input/pridemm-info/PrideMM.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_hate_lr1e3')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Pride'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'hate'\n",
    "cfg.seed = 42\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['No particular target', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 1e-3\n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Humour Classification Seed 42 Lr 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/pridemm/PrideMM/Images'\n",
    "cfg.info_file = '/kaggle/input/pridemm-info/PrideMM.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_humour_lr3e4')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Pride'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'humour'\n",
    "cfg.seed = 42\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['No particular target', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 3e-4\n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Humour Classification Seed 42 Lr 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/pridemm/PrideMM/Images'\n",
    "cfg.info_file = '/kaggle/input/pridemm-info/PrideMM.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_humour_lr1e3')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Pride'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'humour'\n",
    "cfg.seed = 42\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['No particular target', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 1e-3\n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Humour Classification Seed 42 Lr 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/pridemm/PrideMM/Images'\n",
    "cfg.info_file = '/kaggle/input/pridemm-info/PrideMM.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_humour_lr1e5')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Pride'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'humour'\n",
    "cfg.seed = 42\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['No particular target', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 1e-5\n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Classification Seed 42 Lr 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/pridemm/PrideMM/Images'\n",
    "cfg.info_file = '/kaggle/input/pridemm-info/PrideMM.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_target_lr3e4')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Pride'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'target'\n",
    "cfg.seed = 42\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['No particular target', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 3e-4\n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Classification Seed 42 Lr 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/pridemm/PrideMM/Images'\n",
    "cfg.info_file = '/kaggle/input/pridemm-info/PrideMM.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_target_lr1e3')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Pride'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'target'\n",
    "cfg.seed = 42\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['No particular target', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 1e-3\n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Classification Seed 42 Lr 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/pridemm/PrideMM/Images'\n",
    "cfg.info_file = '/kaggle/input/pridemm-info/PrideMM.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_target_lr1e5')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Pride'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'target'\n",
    "cfg.seed = 42\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['No particular target', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 1e-5\n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stance Classification Seed 42 Lr 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/pridemm/PrideMM/Images'\n",
    "cfg.info_file = '/kaggle/input/pridemm-info/PrideMM.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_stance_lr3e4')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Pride'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'stance'\n",
    "cfg.seed = 42\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['No particular target', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 3e-4\n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stance Classification Seed 42 Lr 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/pridemm/PrideMM/Images'\n",
    "cfg.info_file = '/kaggle/input/pridemm-info/PrideMM.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_stance_lr1e3')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Pride'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'stance'\n",
    "cfg.seed = 42\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['No particular target', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 1e-3\n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stance Classification Seed 42 Lr 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/MemeCLIP/code/configs.py\n",
    "import os\n",
    "from yacs.config import CfgNode \n",
    "\n",
    "cfg = CfgNode()\n",
    "cfg.root_dir = '/kaggle/working/'\n",
    "cfg.img_folder = '/kaggle/input/pridemm/PrideMM/Images'\n",
    "cfg.info_file = '/kaggle/input/pridemm-info/PrideMM.csv'\n",
    "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints_stance_lr1e5')\n",
    "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path,'model.ckpt')\n",
    "\n",
    "cfg.clip_variant = \"ViT-L/14\"\n",
    "cfg.dataset_name = 'Pride'\n",
    "cfg.name = 'MemeCLIP' \n",
    "cfg.label = 'stance'\n",
    "cfg.seed = 42\n",
    "cfg.test_only = False\n",
    "cfg.device = 'cuda'\n",
    "cfg.gpus = [0]\n",
    "\n",
    "if cfg.label =='hate':\n",
    "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
    "elif cfg.label == 'humour':\n",
    "    cfg.class_names = ['No Humour', 'Humour']\n",
    "elif cfg.label == 'target':\n",
    "    cfg.class_names = ['No particular target', 'Individual', 'Community', 'Organization']\n",
    "elif cfg.label == 'stance':\n",
    "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
    "  \n",
    "cfg.batch_size = 16\n",
    "cfg.image_size = 224\n",
    "cfg.reproduce = False\n",
    "cfg.num_mapping_layers = 1\n",
    "cfg.unmapped_dim = 768\n",
    "cfg.map_dim = 1024\n",
    "cfg.num_pre_output_layers = 1\n",
    "cfg.drop_probs = [0.1, 0.4, 0.2]\n",
    "cfg.lr = 1e-5\n",
    "cfg.max_epochs = 10\n",
    "cfg.ratio = 0.2\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.num_classes = len(cfg.class_names)\n",
    "cfg.scale = 30 \n",
    "cfg.print_model = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /kaggle/working/MemeCLIP/code/main.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7527542,
     "sourceId": 11970634,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8628266,
     "sourceId": 13580981,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
